import time
import math
from evaluation import evaluate_board, get_game_phase
from constants import piece_score

class ChessBot:
    def __init__(self, ai_turn):
        self.ai_turn = "w" if ai_turn == 0 else "b"
        self.best_move = None
        self.total_nodes = 0
        self.total_branch_cutoff = 0
        self.total_nodes_leaf = 0
        self.execution_time = 0
        self.time_generate_moves = 0
        self.is_endgame = False
        self.DEPTH = 4
        self.algo_search = "Alpha Beta"

    def reset_parameters(self):
        self.best_move = None
        self.total_nodes = 0
        self.total_branch_cutoff = 0
        self.total_nodes_leaf = 0
        self.time_generate_moves = 0

    def get_best_move(self, gs, use_minimax=False):
        self.reset_parameters()
        start_time = time.time()
        self.is_endgame = get_game_phase(gs)
        if use_minimax:
            self.algo_search = "Minimax"
            self.max_score = self.__minimax(gs, self.DEPTH, True)
        else:
            self.algo_search = "Alpha Beta"
            alpha = -math.inf
            beta = math.inf
            self.max_score = self.__alpha_beta_pruning(gs, self.DEPTH, alpha, beta, True)
        self.execution_time = time.time() - start_time
        return self.best_move

    def __sort_moves(self, gs, moves):
        def move_score(move):
            # Ưu tiên nước đi bắt quân (dựa trên giá trị quân bị bắt)
            if move.capturedPiece != "--":
                captured_value = piece_score[move.capturedPiece[1]]
                return captured_value
            # Ưu tiên phát triển quân (mã, tượng)
            if move.movePiece[1] in ["N", "B"]:
                return 50
            return 0

        return sorted(moves, key=move_score, reverse=True)

    def __quiescence_search(self, gs, alpha, beta, maximizing_player):
        self.total_nodes_leaf += 1
        eval_score = evaluate_board(gs, self.ai_turn, self.is_endgame)

        if maximizing_player:
            if eval_score >= beta:
                return beta
            alpha = max(alpha, eval_score)
        else:
            if eval_score <= alpha:
                return alpha
            beta = min(beta, eval_score)

        # Chỉ tìm kiếm các nước bắt quân
        capture_moves = [move for move in gs.getValidMoves() if move.capturedPiece != "--"]
        if not capture_moves:
            return eval_score

        capture_moves = self.__sort_moves(gs, capture_moves)

        if maximizing_player:
            for move in capture_moves:
                gs.makeMove(move)
                self.total_nodes += 1
                score = self.__quiescence_search(gs, alpha, beta, False)
                gs.undoMove()
                if score >= beta:
                    self.total_branch_cutoff += 1
                    return beta
                alpha = max(alpha, score)
            return alpha
        else:
            for move in capture_moves:
                gs.makeMove(move)
                self.total_nodes += 1
                score = self.__quiescence_search(gs, alpha, beta, True)
                gs.undoMove()
                if score <= alpha:
                    self.total_branch_cutoff += 1
                    return alpha
                beta = min(beta, score)
            return beta

    def __minimax(self, gs, depth, maximizing_player):
        if depth == 0 or len(gs.getValidMoves()) == 0:
            self.total_nodes_leaf += 1
            return evaluate_board(gs, self.ai_turn, self.is_endgame)

        moves = self.__sort_moves(gs, gs.getValidMoves())
        if maximizing_player:
            max_eval = -math.inf
            for move in moves:
                gs.makeMove(move)
                self.total_nodes += 1
                eval_score = self.__minimax(gs, depth - 1, False)
                gs.undoMove()
                if eval_score > max_eval:
                    max_eval = eval_score
                    if depth == self.DEPTH:
                        self.best_move = move
            return max_eval
        else:
            min_eval = math.inf
            for move in moves:
                gs.makeMove(move)
                self.total_nodes += 1
                eval_score = self.__minimax(gs, depth - 1, True)
                gs.undoMove()
                if eval_score < min_eval:
                    min_eval = eval_score
                    if depth == self.DEPTH:
                        self.best_move = move
            return min_eval

    def __alpha_beta_pruning(self, gs, depth, alpha, beta, maximizing_player):
        if depth == 0 or len(gs.getValidMoves()) == 0:
            return self.__quiescence_search(gs, alpha, beta, maximizing_player)

        start = time.time()
        moves = self.__sort_moves(gs, gs.getValidMoves())
        self.time_generate_moves += time.time() - start

        if maximizing_player:
            max_eval = -math.inf
            for move in moves:
                gs.makeMove(move)
                self.total_nodes += 1
                eval_score = self.__alpha_beta_pruning(gs, depth - 1, alpha, beta, False)
                gs.undoMove()
                if eval_score > max_eval:
                    max_eval = eval_score
                    if depth == self.DEPTH:
                        self.best_move = move
                alpha = max(alpha, eval_score)
                if beta <= alpha:
                    self.total_branch_cutoff += 1
                    break
            return max_eval
        else:
            min_eval = math.inf
            for move in moves:
                gs.makeMove(move)
                self.total_nodes += 1
                eval_score = self.__alpha_beta_pruning(gs, depth - 1, alpha, beta, True)
                gs.undoMove()
                if eval_score < min_eval:
                    min_eval = eval_score
                    if depth == self.DEPTH:
                        self.best_move = move
                beta = min(beta, eval_score)
                if beta <= alpha:
                    self.total_branch_cutoff += 1
                    break
            return min_eval